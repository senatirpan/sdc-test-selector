{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Janik\n"
     ]
    }
   ],
   "source": [
    "!pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Conv1D, MaxPooling1D, Bidirectional, LSTM, \n",
    "                                   Dense, Dropout, Flatten, BatchNormalization,\n",
    "                                   GlobalMaxPooling1D, Concatenate, Input)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "\n",
    "def calculate_angle(p1, p2, p3):\n",
    "    \"\"\"Calculate angle between three points with improved handling\"\"\"\n",
    "    v1 = np.array([p2[0] - p1[0], p2[1] - p1[1]])\n",
    "    v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "    \n",
    "    # Handle zero vectors\n",
    "    if np.allclose(v1, 0) or np.allclose(v2, 0):\n",
    "        return 0\n",
    "    \n",
    "    # Calculate angle with numerical stability\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norms = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    \n",
    "    if norms == 0:\n",
    "        return 0\n",
    "    \n",
    "    cos_angle = np.clip(dot_product / norms, -1.0, 1.0)\n",
    "    angle = np.arccos(cos_angle)\n",
    "    \n",
    "    # Convert to degrees and return as turning angle\n",
    "    angle_deg = np.degrees(angle)\n",
    "    return 180 - angle_deg\n",
    "\n",
    "\n",
    "def calculate_curvature(p1, p2, p3):\n",
    "    \"\"\"Calculate curvature at point p2\"\"\"\n",
    "    # Using the formula: curvature = |cross_product| / |v|^3\n",
    "    v1 = np.array([p2[0] - p1[0], p2[1] - p1[1]])\n",
    "    v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n",
    "    \n",
    "    if np.allclose(v1, 0) or np.allclose(v2, 0):\n",
    "        return 0\n",
    "    \n",
    "    # Cross product for 2D vectors\n",
    "    cross = v1[0] * v2[1] - v1[1] * v2[0]\n",
    "    v_norm = np.linalg.norm(v1 + v2)\n",
    "    \n",
    "    if v_norm == 0:\n",
    "        return 0\n",
    "    \n",
    "    return abs(cross) / (v_norm ** 3)\n",
    "\n",
    "\n",
    "def calculate_segment_length(p1, p2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
    "\n",
    "\n",
    "def extract_three_features(road_coordinates):\n",
    "    \"\"\"Extract three features: angles, lengths, and curvature\"\"\"\n",
    "    if len(road_coordinates) < 3:\n",
    "        return [], [], []\n",
    "    \n",
    "    angles = []\n",
    "    lengths = []\n",
    "    curvatures = []\n",
    "    \n",
    "    # Calculate segment lengths\n",
    "    for i in range(len(road_coordinates) - 1):\n",
    "        length = calculate_segment_length(road_coordinates[i], road_coordinates[i + 1])\n",
    "        lengths.append(length)\n",
    "    \n",
    "    # Calculate angles and curvatures (need at least 3 points)\n",
    "    for i in range(len(road_coordinates) - 2):\n",
    "        angle = calculate_angle(road_coordinates[i], road_coordinates[i + 1], road_coordinates[i + 2])\n",
    "        angles.append(angle)\n",
    "        \n",
    "        curvature = calculate_curvature(road_coordinates[i], road_coordinates[i + 1], road_coordinates[i + 2])\n",
    "        curvatures.append(curvature)\n",
    "    \n",
    "    # Pad to match dimensions with lengths (which has one more element)\n",
    "    while len(angles) < len(lengths):\n",
    "        angles.append(angles[-1] if angles else 0)\n",
    "    while len(curvatures) < len(lengths):\n",
    "        curvatures.append(curvatures[-1] if curvatures else 0)\n",
    "    \n",
    "    return angles, lengths, curvatures\n",
    "\n",
    "\n",
    "def load_and_preprocess_data_three_features(data_dir, max_files=None, min_sequence_length=10):\n",
    "    \"\"\"Load JSON files and extract three features\"\"\"\n",
    "    print(\"Loading and preprocessing data with three features (angles, lengths, curvature)...\")\n",
    "    \n",
    "    X_angles = []\n",
    "    X_lengths = []\n",
    "    X_curvatures = []\n",
    "    y = []\n",
    "    \n",
    "    json_files = [f for f in os.listdir(data_dir) if f.endswith('.json') and f != 'ReadMe.txt']\n",
    "    json_files.sort()\n",
    "    \n",
    "    if max_files:\n",
    "        json_files = json_files[:max_files]\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files to process\")\n",
    "    \n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, file in enumerate(json_files):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Processing file {i+1}/{len(json_files)}: {file}\")\n",
    "        \n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            road_coordinates = data.get('road_points', [])\n",
    "            test_outcome = data.get('test_outcome', 'PASS')\n",
    "            \n",
    "            if not road_coordinates:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            label_binary = 1 if test_outcome == 'FAIL' else 0\n",
    "            \n",
    "            # Clean road coordinates\n",
    "            clean_coordinates = []\n",
    "            for point in road_coordinates:\n",
    "                if isinstance(point, list) and len(point) >= 2:\n",
    "                    clean_coordinates.append([float(point[0]), float(point[1])])\n",
    "            \n",
    "            # Only process if we have enough points\n",
    "            if len(clean_coordinates) >= min_sequence_length:\n",
    "                angles, lengths, curvatures = extract_three_features(clean_coordinates)\n",
    "                \n",
    "                if len(angles) > 0 and len(lengths) > 0 and len(curvatures) > 0:\n",
    "                    X_angles.append(angles)\n",
    "                    X_lengths.append(lengths)\n",
    "                    X_curvatures.append(curvatures)\n",
    "                    y.append(label_binary)\n",
    "                    processed += 1\n",
    "                else:\n",
    "                    skipped += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            if i < 5:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {processed} files\")\n",
    "    print(f\"Skipped: {skipped} files\")\n",
    "    print(f\"Total samples loaded: {len(X_angles)}\")\n",
    "    \n",
    "    if y:\n",
    "        pass_count = sum(1 for label in y if label == 0)\n",
    "        fail_count = sum(1 for label in y if label == 1)\n",
    "        print(f\"Label distribution - PASS: {pass_count}, FAIL: {fail_count}\")\n",
    "    \n",
    "    return X_angles, X_lengths, X_curvatures, y\n",
    "\n",
    "\n",
    "def pad_sequences_robust(sequences, max_length=None, padding='post'):\n",
    "    \"\"\"Robust sequence padding with better handling\"\"\"\n",
    "    if not sequences:\n",
    "        return np.array([]), 0\n",
    "        \n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences if len(seq) > 0)\n",
    "    \n",
    "    # Cap max_length to reasonable value\n",
    "    max_length = min(max_length, 500)  # Prevent memory issues\n",
    "    \n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= max_length:\n",
    "            # Take the middle part of the sequence for better representation\n",
    "            start_idx = (len(seq) - max_length) // 2\n",
    "            padded.append(seq[start_idx:start_idx + max_length])\n",
    "        else:\n",
    "            if padding == 'post':\n",
    "                padded.append(seq + [0] * (max_length - len(seq)))\n",
    "            else:\n",
    "                padded.append([0] * (max_length - len(seq)) + seq)\n",
    "    \n",
    "    return np.array(padded), max_length\n",
    "\n",
    "\n",
    "def create_three_feature_hybrid_model(input_shape, num_cnn_filters=128, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Create hybrid 1D CNN + BiLSTM model for three features\n",
    "    \n",
    "    Architecture Explanation:\n",
    "    1. Input Layer: Receives sequences of shape (sequence_length, 3_features)\n",
    "    \n",
    "    2. Multi-scale CNN Feature Extraction:\n",
    "       - Scale 1 (Fine-grained): kernel_size=3 for capturing local patterns\n",
    "       - Scale 2 (Medium-scale): kernel_size=5 for medium-range dependencies  \n",
    "       - Scale 3 (Large-scale): kernel_size=7 for long-range patterns\n",
    "       Each scale has its own convolutional layers with batch normalization\n",
    "    \n",
    "    3. Feature Concatenation: Combines multi-scale CNN features\n",
    "    \n",
    "    4. Bidirectional LSTM Layers:\n",
    "       - First BiLSTM: Captures temporal dependencies in both directions\n",
    "       - Second BiLSTM: Further processes sequential patterns\n",
    "       - Returns sequences for first layer, final states for second layer\n",
    "    \n",
    "    5. Global Pooling Path: Parallel extraction of global features from CNN\n",
    "    \n",
    "    6. Feature Fusion: Combines LSTM outputs with global pooling features\n",
    "    \n",
    "    7. Dense Classification Layers:\n",
    "       - Multiple dense layers with regularization (L1+L2)\n",
    "       - Batch normalization and dropout for preventing overfitting\n",
    "       - Final sigmoid output for binary classification\n",
    "    \n",
    "    This architecture captures both:\n",
    "    - Local geometric patterns (CNN with multiple scales)\n",
    "    - Temporal road trajectory patterns (BiLSTM)\n",
    "    - Global road characteristics (Global pooling)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer for 3 features\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Multi-scale CNN feature extraction\n",
    "    # Scale 1: Fine-grained patterns (kernel=3)\n",
    "    conv1 = Conv1D(filters=num_cnn_filters, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv1D(filters=num_cnn_filters//2, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    \n",
    "    # Scale 2: Medium-scale patterns (kernel=5)\n",
    "    conv2 = Conv1D(filters=num_cnn_filters//2, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv1D(filters=num_cnn_filters//4, kernel_size=5, activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    \n",
    "    # Scale 3: Large-scale patterns (kernel=7)\n",
    "    conv3 = Conv1D(filters=num_cnn_filters//4, kernel_size=7, activation='relu', padding='same')(inputs)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "    \n",
    "    # Concatenate multi-scale features\n",
    "    concat = Concatenate()([pool1, pool2, pool3])\n",
    "    concat = Dropout(0.3)(concat)\n",
    "    \n",
    "    # Bidirectional LSTM layers for temporal pattern recognition\n",
    "    lstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.2, \n",
    "                              recurrent_dropout=0.2))(concat)\n",
    "    lstm1 = BatchNormalization()(lstm1)\n",
    "    \n",
    "    lstm2 = Bidirectional(LSTM(lstm_units//2, return_sequences=False, dropout=0.2, \n",
    "                              recurrent_dropout=0.2))(lstm1)\n",
    "    lstm2 = BatchNormalization()(lstm2)\n",
    "    \n",
    "    # Global pooling of CNN features (parallel path)\n",
    "    global_pool = GlobalMaxPooling1D()(concat)\n",
    "    \n",
    "    # Combine LSTM and global pooling features\n",
    "    combined = Concatenate()([lstm2, global_pool])\n",
    "    combined = Dropout(0.5)(combined)\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    dense1 = Dense(64, activation='relu', \n",
    "                   kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(combined)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Dropout(0.5)(dense1)\n",
    "    \n",
    "    dense2 = Dense(32, activation='relu', \n",
    "                   kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Dropout(0.3)(dense2)\n",
    "    \n",
    "    # Output layer for binary classification\n",
    "    outputs = Dense(1, activation='sigmoid')(dense2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_three_feature_model_kfold(data_dir, n_splits=10, random_state=42, max_files=None):\n",
    "    \"\"\"Main training function with k-fold cross-validation for three features\"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_angles, X_lengths, X_curvatures, y = load_and_preprocess_data_three_features(\n",
    "        data_dir, max_files=max_files)\n",
    "    \n",
    "    if len(X_angles) == 0:\n",
    "        print(\"No data loaded. Please check your data directory and file format.\")\n",
    "        return\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_angles_padded, max_len_angles = pad_sequences_robust(X_angles)\n",
    "    X_lengths_padded, max_len_lengths = pad_sequences_robust(X_lengths)\n",
    "    X_curvatures_padded, max_len_curvatures = pad_sequences_robust(X_curvatures)\n",
    "    \n",
    "    # Ensure all features have the same sequence length\n",
    "    max_len = max(max_len_angles, max_len_lengths, max_len_curvatures)\n",
    "    \n",
    "    def pad_to_max_len(arr, current_len, target_len):\n",
    "        if current_len < target_len:\n",
    "            return np.pad(arr, ((0, 0), (0, target_len - current_len)), \n",
    "                         mode='constant', constant_values=0)\n",
    "        return arr\n",
    "    \n",
    "    X_angles_padded = pad_to_max_len(X_angles_padded, max_len_angles, max_len)\n",
    "    X_lengths_padded = pad_to_max_len(X_lengths_padded, max_len_lengths, max_len)\n",
    "    X_curvatures_padded = pad_to_max_len(X_curvatures_padded, max_len_curvatures, max_len)\n",
    "    \n",
    "    # Stack features (3 features: angles, lengths, curvatures)\n",
    "    X = np.stack([X_angles_padded, X_lengths_padded, X_curvatures_padded], axis=-1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"Final data shape: {X.shape}\")\n",
    "    print(f\"Sequence length: {max_len}\")\n",
    "    print(f\"Number of features: {X.shape[2]} (angles, lengths, curvature)\")\n",
    "    print(f\"Labels distribution - PASS: {np.sum(y == 0)}, FAIL: {np.sum(y == 1)}\")\n",
    "    \n",
    "    # K-fold cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    fold_results = []\n",
    "    fold_histories = []\n",
    "    fold_models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training Fold {fold + 1}/{n_splits}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Standardize features for this fold\n",
    "        scaler = RobustScaler()  # More robust to outliers than StandardScaler\n",
    "        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "        X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
    "        X_val_scaled = scaler.transform(X_val_reshaped).reshape(X_val.shape)\n",
    "        \n",
    "        # Calculate class weights for this fold\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "        \n",
    "        print(f\"Fold {fold + 1} - Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "        print(f\"Class weights: {class_weight_dict}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_three_feature_hybrid_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "        \n",
    "        # Compile with AdamW optimizer\n",
    "        model.compile(\n",
    "            optimizer=AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=15, \n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.3, \n",
    "            patience=8, \n",
    "            min_lr=0.00001,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(\n",
    "            f'best_model_fold_{fold + 1}.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=150,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_val_pred_proba = model.predict(X_val_scaled)\n",
    "        y_val_pred = (y_val_pred_proba > 0.5).astype(int).flatten()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        precision = precision_score(y_val, y_val_pred)\n",
    "        recall = recall_score(y_val, y_val_pred)\n",
    "        f1 = f1_score(y_val, y_val_pred)\n",
    "        auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "        \n",
    "        fold_result = {\n",
    "            'fold': fold + 1,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'confusion_matrix': confusion_matrix(y_val, y_val_pred),\n",
    "            'scaler': scaler  # Save scaler for inference\n",
    "        }\n",
    "        \n",
    "        fold_results.append(fold_result)\n",
    "        fold_histories.append(history)\n",
    "        fold_models.append({'model': model, 'scaler': scaler})\n",
    "        \n",
    "        print(f\"\\nFold {fold + 1} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{fold_result['confusion_matrix']}\")\n",
    "    \n",
    "    # Find best performing fold\n",
    "    best_fold_idx = np.argmax([r['f1'] for r in fold_results])  # Using F1-score as metric\n",
    "    best_fold_result = fold_results[best_fold_idx]\n",
    "    best_model_info = fold_models[best_fold_idx]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BEST PERFORMING FOLD: {best_fold_result['fold']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best Fold F1-Score: {best_fold_result['f1']:.4f}\")\n",
    "    print(f\"Best Fold Accuracy: {best_fold_result['accuracy']:.4f}\")\n",
    "    print(f\"Best Fold AUC: {best_fold_result['auc']:.4f}\")\n",
    "    \n",
    "    # Save best model as ONNX\n",
    "    try:\n",
    "        print(\"\\nSaving best model as ONNX...\")\n",
    "        # Create a sample input for ONNX conversion\n",
    "        sample_input = tf.TensorSpec(shape=(None, X.shape[1], X.shape[2]), dtype=tf.float32)\n",
    "        \n",
    "        # Convert to ONNX\n",
    "        onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "            best_model_info['model'],\n",
    "            input_signature=[sample_input],\n",
    "            opset=13\n",
    "        )\n",
    "        \n",
    "        # Save ONNX model\n",
    "        with open('best_model.onnx', 'wb') as f:\n",
    "            f.write(onnx_model.SerializeToString())\n",
    "        \n",
    "        print(\"✓ Best model saved as 'best_model.onnx'\")\n",
    "        \n",
    "        # Save the scaler for the best model\n",
    "        import joblib\n",
    "        joblib.dump(best_model_info['scaler'], 'best_model_scaler.pkl')\n",
    "        print(\"✓ Best model scaler saved as 'best_model_scaler.pkl'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving ONNX model: {e}\")\n",
    "        print(\"Saving best model as .h5 instead...\")\n",
    "        best_model_info['model'].save('best_model_backup.h5')\n",
    "    \n",
    "    # Calculate average results\n",
    "    avg_results = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in fold_results]),\n",
    "        'precision': np.mean([r['precision'] for r in fold_results]),\n",
    "        'recall': np.mean([r['recall'] for r in fold_results]),\n",
    "        'f1': np.mean([r['f1'] for r in fold_results]),\n",
    "        'auc': np.mean([r['auc'] for r in fold_results])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"K-FOLD CROSS-VALIDATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average Accuracy:  {avg_results['accuracy']:.4f} ± {np.std([r['accuracy'] for r in fold_results]):.4f}\")\n",
    "    print(f\"Average Precision: {avg_results['precision']:.4f} ± {np.std([r['precision'] for r in fold_results]):.4f}\")\n",
    "    print(f\"Average Recall:    {avg_results['recall']:.4f} ± {np.std([r['recall'] for r in fold_results]):.4f}\")\n",
    "    print(f\"Average F1-Score:  {avg_results['f1']:.4f} ± {np.std([r['f1'] for r in fold_results]):.4f}\")\n",
    "    print(f\"Average AUC:       {avg_results['auc']:.4f} ± {np.std([r['auc'] for r in fold_results]):.4f}\")\n",
    "    \n",
    "    print(f\"\\n✓ All fold models saved as 'best_model_fold_X.h5' files\")\n",
    "    \n",
    "    return fold_results, avg_results, best_fold_result\n",
    "\n",
    "\n",
    "def preprocess_inference_data(road_points_data):\n",
    "    \"\"\"\n",
    "    Preprocess inference data to match training format\n",
    "    Handles the new JSON format with 'x' and 'y' keys\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for sample in road_points_data:\n",
    "        road_points = sample.get('road_points', [])\n",
    "        \n",
    "        # Convert from {\"x\": val, \"y\": val} format to [x, y] format\n",
    "        clean_coordinates = []\n",
    "        for point in road_points:\n",
    "            if isinstance(point, dict) and 'x' in point and 'y' in point:\n",
    "                clean_coordinates.append([float(point['x']), float(point['y'])])\n",
    "            elif isinstance(point, list) and len(point) >= 2:\n",
    "                clean_coordinates.append([float(point[0]), float(point[1])])\n",
    "        \n",
    "        if len(clean_coordinates) >= 3:  # Need at least 3 points for features\n",
    "            angles, lengths, curvatures = extract_three_features(clean_coordinates)\n",
    "            \n",
    "            if len(angles) > 0 and len(lengths) > 0 and len(curvatures) > 0:\n",
    "                processed_data.append({\n",
    "                    'id': sample.get('_id', {}).get('$oid', 'unknown'),\n",
    "                    'angles': angles,\n",
    "                    'lengths': lengths,\n",
    "                    'curvatures': curvatures\n",
    "                })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Example inference function\n",
    "def predict_road_outcomes(inference_file_path, model_path='best_model.onnx', scaler_path='best_model_scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Predict outcomes for new road data\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Load the inference data\n",
    "    with open(inference_file_path, 'r') as f:\n",
    "        inference_data = json.load(f)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    processed_data = preprocess_inference_data(inference_data)\n",
    "    \n",
    "    if not processed_data:\n",
    "        print(\"No valid data found for inference\")\n",
    "        return []\n",
    "    \n",
    "    # Extract features\n",
    "    X_angles = [item['angles'] for item in processed_data]\n",
    "    X_lengths = [item['lengths'] for item in processed_data]\n",
    "    X_curvatures = [item['curvatures'] for item in processed_data]\n",
    "    \n",
    "    # Pad sequences (use same max_length as training - you'll need to save this value)\n",
    "    X_angles_padded, _ = pad_sequences_robust(X_angles, max_length=500)  # Use training max_length\n",
    "    X_lengths_padded, _ = pad_sequences_robust(X_lengths, max_length=500)\n",
    "    X_curvatures_padded, _ = pad_sequences_robust(X_curvatures, max_length=500)\n",
    "    \n",
    "    # Stack features\n",
    "    X = np.stack([X_angles_padded, X_lengths_padded, X_curvatures_padded], axis=-1)\n",
    "    \n",
    "    # Load scaler and scale features\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "    X_scaled = scaler.transform(X_reshaped).reshape(X.shape)\n",
    "    \n",
    "    # Load ONNX model and predict\n",
    "    ort_session = ort.InferenceSession(model_path)\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    predictions = ort_session.run(None, {input_name: X_scaled.astype(np.float32)})[0]\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i, item in enumerate(processed_data):\n",
    "        pred_prob = predictions[i][0]\n",
    "        pred_outcome = \"FAIL\" if pred_prob > 0.5 else \"PASS\"\n",
    "        results.append({\n",
    "            'id': item['id'],\n",
    "            'predicted_outcome': pred_outcome,\n",
    "            'confidence': float(pred_prob)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set data directory path\n",
    "    data_dir = \"/Users/senatarpan/Desktop/repos/sdc-testing-competition/tools/my_tool/data/raw_data\"\n",
    "    \n",
    "    # Use full dataset for maximum performance\n",
    "    max_files = None  # Use all 10,000 files\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING 3-FEATURE HYBRID CNN+BiLSTM MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Training with {'all 10,000' if max_files is None else 'first ' + str(max_files)} files...\")\n",
    "    print(\"\\nFeatures used:\")\n",
    "    print(\"1. Turning Angles - Angular changes in road trajectory\")\n",
    "    print(\"2. Segment Lengths - Distance between consecutive road points\") \n",
    "    print(\"3. Curvature - Road curvature at each point\")\n",
    "    print(\"\\nModel improvements:\")\n",
    "    print(\"- K-fold cross-validation (10 folds)\")\n",
    "    print(\"- Multi-scale CNN architecture (3, 5, 7 kernel sizes)\")\n",
    "    print(\"- Bidirectional LSTM for temporal patterns\")\n",
    "    print(\"- Feature fusion (CNN + LSTM + Global pooling)\")\n",
    "    print(\"- Class weighting for imbalanced data\")\n",
    "    print(\"- Robust scaling and regularization\")\n",
    "    print(\"- AdamW optimizer with weight decay\")\n",
    "    \n",
    "    # Train the model\n",
    "    fold_results, avg_results, best_fold_result = train_three_feature_model_kfold(data_dir, max_files=max_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
